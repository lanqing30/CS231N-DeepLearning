{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n",
    "                     subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "      mean_image = np.mean(X_train, axis=0)\n",
    "      X_train -= mean_image\n",
    "      X_val -= mean_image\n",
    "      X_test -= mean_image\n",
    "    \n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_val': X_val, 'y_val': y_val,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully connected layers and relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M) D = d_1 * d_2 * d_3 ... d_4\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    x_flatten = x.reshape(x.shape[0], -1) # (N, D)\n",
    "    out = np.dot(x_flatten, w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    x_flatten = x.reshape(x.shape[0], -1)\n",
    "    x_shape = x.shape\n",
    "    N, D = x_flatten.shape\n",
    "    dx = np.dot(dout, w.T)\n",
    "    db = np.sum(dout, axis = 0)\n",
    "    dw = np.dot(x_flatten.T, dout)\n",
    "    dx = dx.reshape(*x_shape)\n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "    Input:\n",
    "        - x: Inputs, of any shape\n",
    "    Returns a tuple of:\n",
    "        - out: Output, of the same shape as x\n",
    "        - cache: x\n",
    "    \"\"\"\n",
    "    out = np.array(x)\n",
    "    out[out < 0] = 0\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "    Input:\n",
    "        - dout: Upstream derivatives, of any shape\n",
    "        - cache: Input x, of same shape as dout\n",
    "    Returns:\n",
    "        - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    dx = np.array(dout)\n",
    "    dx[x < 0] = 0\n",
    "    return dx\n",
    "\n",
    "def affine_relu_forward(x, w, b):\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def affine_relu_backward(dout, cache):\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduce svm-loss and softmax-loss\n",
    "\n",
    "deraivative to s instead of w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient using for multiclass SVM classification.\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    correct_class_scores = x[np.arange(N), y]\n",
    "    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "    margins[np.arange(N), y] = 0\n",
    "    loss = np.sum(margins) / N\n",
    "    num_pos = np.sum(margins > 0, axis=1)\n",
    "    dx = np.zeros_like(x)\n",
    "    dx[margins > 0] = 1\n",
    "    dx[np.arange(N), y] -= num_pos\n",
    "    dx /= N\n",
    "    return loss, dx\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch normalization\n",
    "\n",
    "![](img/bn_forward.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "    ----------------------------------------------------------------------------\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "      \n",
    "    out, cache = None, None\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "        # running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "        mean = np.mean(x, axis=0)\n",
    "        var = np.var(x, axis=0)\n",
    "        x_mu = x - mean\n",
    "        inv_var = 1.0 / np.sqrt(var + eps)\n",
    "        x_hat = x_mu * inv_var\n",
    "        out = gamma*x_hat + beta\n",
    "        \n",
    "        running_mean = momentum * running_mean + (1 - momentum) * mean\n",
    "        running_var = momentum * running_var + (1 - momentum) * var\n",
    "        \n",
    "        cache = x_mu, inv_var, x_hat, gamma \n",
    "    \n",
    "    elif mode == 'test':\n",
    "        x_normalize = (x - running_mean) / np.sqrt(running_var + eps)\n",
    "        out = gamma * x_normalize + beta  \n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bn_back.jpg)\n",
    "\n",
    "(1):obviously.\n",
    "\n",
    "(2):\n",
    "\n",
    "![](img/bn_back_2.svg)\n",
    "\n",
    "(3):\n",
    "\n",
    "(4):\n",
    "\n",
    "![](img/bn_back_3.svg)\n",
    "\n",
    "![](img/bn_back_4.svg)\n",
    "\n",
    "(5):obviously\n",
    "\n",
    "(6):obviously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for batch normalization.\n",
    "\n",
    "    For this implementation you should work out the derivatives for the batch\n",
    "    normalizaton backward pass on paper and simplify as much as possible. You\n",
    "    should be able to derive a simple expression for the backward pass.\n",
    "\n",
    "    Note: This implementation should expect to receive the same cache variable\n",
    "    as batchnorm_backward, but might not use all of the values in the cache.\n",
    "\n",
    "    Inputs / outputs: Same as batchnorm_backward\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = dout.shape\n",
    "    x_mu, inv_var, x_hat, gamma = cache\n",
    "\n",
    "    # intermediate partial derivatives\n",
    "    dxhat = dout * gamma\n",
    "    dvar = np.sum((dxhat * x_mu * (-0.5) * (inv_var)**3), axis=0)\n",
    "    dmu = (np.sum((dxhat * -inv_var), axis=0)) + (dvar * (-2.0 / N) * np.sum(x_mu, axis=0))\n",
    "    dx1 = dxhat * inv_var\n",
    "    dx2 = dvar * (2.0 / N) * x_mu\n",
    "    dx3 = (1.0 / N) * dmu\n",
    "\n",
    "    # final partial derivatives\n",
    "    dx = dx1 + dx2 + dx3\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(x_hat*dout, axis=0)\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(x, dropout_param):\n",
    "  \"\"\"\n",
    "  Performs the forward pass for (inverted) dropout.\n",
    "  Inputs:\n",
    "  - x: Input data, of any shape\n",
    "  - dropout_param: A dictionary with the following keys:\n",
    "    - p: Dropout parameter. We drop each neuron output with probability p.\n",
    "    - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "      if the mode is test, then just return the input.\n",
    "    - seed: Seed for the random number generator. Passing seed makes this\n",
    "      function deterministic, which is needed for gradient checking but not in\n",
    "      real networks.\n",
    "  Outputs:\n",
    "  - out: Array of the same shape as x.\n",
    "  - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "    mask that was used to multiply the input; in test mode, mask is None.\n",
    "  \"\"\"\n",
    "  p, mode = dropout_param['p'], dropout_param['mode']\n",
    "  if 'seed' in dropout_param:\n",
    "    np.random.seed(dropout_param['seed'])\n",
    "\n",
    "  mask = None\n",
    "  out = None\n",
    "\n",
    "  if mode == 'train':\n",
    "    mask = (np.random.rand(*x.shape) >= p) / (1 - p)\n",
    "    out = x * mask\n",
    "  elif mode == 'test':\n",
    "    out = x\n",
    "\n",
    "  cache = (dropout_param, mask)\n",
    "  out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Perform the backward pass for (inverted) dropout.\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: (dropout_param, mask) from dropout_forward.\n",
    "  \"\"\"\n",
    "  dropout_param, mask = cache\n",
    "  mode = dropout_param['mode']\n",
    "  dx = None\n",
    "  if mode == 'train':\n",
    "    dx = dout * mask\n",
    "  elif mode == 'test':\n",
    "    dx = dout\n",
    "  return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine fc-bn-relu layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_bn_relu_forward(x, w, b, gamma, beta, bn_params):\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    bn_out, bn_cache = batchnorm_forward(a, gamma, beta, bn_params)\n",
    "    out, relu_cache = relu_forward(bn_out)\n",
    "    cache = [fc_cache, relu_cache, bn_cache]\n",
    "    return out, cache\n",
    "\n",
    "def affine_bn_relu_backward(dout, cache):\n",
    "    fc_cache, relu_cache, bn_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    d_bacth, dgamma, dbeta = batchnorm_backward_alt(da, bn_cache)\n",
    "    dx, dw, db = affine_backward(d_bacth, fc_cache)\n",
    "    return dx, dw, db, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "  \"\"\"\n",
    "  A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "  ReLU nonlinearities, and a softmax loss function. This will also implement\n",
    "  dropout and batch normalization as options. For a network with L layers,\n",
    "  the architecture will be\n",
    "  \n",
    "  {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "  \n",
    "  where batch normalization and dropout are optional, and the {...} block is\n",
    "  repeated L - 1 times.\n",
    "  \n",
    "  Similar to the TwoLayerNet above, learnable parameters are stored in the\n",
    "  self.params dictionary and will be learned using the Solver class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n",
    "               dropout=0, use_batchnorm=False, reg=0.0,\n",
    "               weight_scale=1e-2, dtype=np.float32, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize a new FullyConnectedNet.\n",
    "    \n",
    "    Inputs:\n",
    "    - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "    - input_dim: An integer giving the size of the input.\n",
    "    - num_classes: An integer giving the number of classes to classify.\n",
    "    - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\n",
    "      the network should not use dropout at all.\n",
    "    - use_batchnorm: Whether or not the network should use batch normalization.\n",
    "    - reg: Scalar giving L2 regularization strength.\n",
    "    - weight_scale: Scalar giving the standard deviation for random\n",
    "      initialization of the weights.\n",
    "    - dtype: A numpy datatype object; all computations will be performed using\n",
    "      this datatype. float32 is faster but less accurate, so you should use\n",
    "      float64 for numeric gradient checking.\n",
    "    - seed: If not None, then pass this random seed to the dropout layers. This\n",
    "      will make the dropout layers deteriminstic so we can gradient check the\n",
    "      model.\n",
    "    \"\"\"\n",
    "    self.use_batchnorm = use_batchnorm\n",
    "    self.use_dropout = dropout > 0\n",
    "    self.reg = reg\n",
    "    self.num_layers = 1 + len(hidden_dims)\n",
    "    self.dtype = dtype\n",
    "    self.params = {}\n",
    "    \n",
    "    layer_input_dim = input_dim\n",
    "    for i, hd in enumerate(hidden_dims):\n",
    "        self.params['W%d'%(i+1)] = weight_scale * np.random.randn(layer_input_dim, hd)\n",
    "        self.params['b%d'%(i+1)] = weight_scale * np.zeros(hd)\n",
    "        if self.use_batchnorm:\n",
    "            self.params['gamma%d'%(i+1)] = np.ones(hd)\n",
    "            self.params['beta%d'%(i+1)] = np.zeros(hd)\n",
    "        layer_input_dim = hd\n",
    "    self.params['W%d'%(self.num_layers)] = weight_scale * np.random.randn(layer_input_dim, num_classes)\n",
    "    self.params['b%d'%(self.num_layers)] = weight_scale * np.zeros(num_classes)\n",
    "    \n",
    "    # When using dropout we need to pass a dropout_param dictionary to each\n",
    "    # dropout layer so that the layer knows the dropout probability and the mode\n",
    "    # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "    self.dropout_param = {}\n",
    "    if self.use_dropout:\n",
    "      self.dropout_param = {'mode': 'train', 'p': dropout}\n",
    "      if seed is not None:\n",
    "        self.dropout_param['seed'] = seed\n",
    "    \n",
    "    # With batch normalization we need to keep track of running means and\n",
    "    # variances, so we need to pass a special bn_param object to each batch\n",
    "    # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "    # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "    # pass of the second batch normalization layer, etc.\n",
    "    self.bn_params = []\n",
    "    if self.use_batchnorm:\n",
    "      self.bn_params = [{'mode': 'train'} for i in xrange(self.num_layers - 1)]\n",
    "    \n",
    "    # Cast all parameters to the correct datatype\n",
    "    for k, v in self.params.iteritems():\n",
    "      self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None):\n",
    "    \"\"\"\n",
    "    Compute loss and gradient for the fully-connected net.\n",
    "    Input / output: Same as TwoLayerNet above.\n",
    "    \"\"\"\n",
    "    X = X.astype(self.dtype)\n",
    "    mode = 'test' if y is None else 'train'\n",
    "\n",
    "    # Set train/test mode for batchnorm params and dropout param since they\n",
    "    # behave differently during training and testing.\n",
    "    if self.use_dropout:\n",
    "      self.dropout_param['mode'] = mode   \n",
    "    if self.use_batchnorm:\n",
    "      for bn_param in self.bn_params:\n",
    "        bn_param['mode'] = mode\n",
    "\n",
    "    scores = None\n",
    "    \n",
    "    layer_input = X\n",
    "    ar_cache = {}\n",
    "    dp_cache = {}\n",
    "    \n",
    "    for lay in xrange(self.num_layers-1):\n",
    "        if self.use_batchnorm:\n",
    "            layer_input, ar_cache[lay] = affine_bn_relu_forward(layer_input, \n",
    "                                        self.params['W%d'%(lay+1)], self.params['b%d'%(lay+1)], \n",
    "                                        self.params['gamma%d'%(lay+1)], self.params['beta%d'%(lay+1)], self.bn_params[lay])\n",
    "        else:\n",
    "            layer_input, ar_cache[lay] = affine_relu_forward(layer_input, self.params['W%d'%(lay+1)], self.params['b%d'%(lay+1)])\n",
    "            \n",
    "        if self.use_dropout:\n",
    "            layer_input,  dp_cache[lay] = dropout_forward(layer_input, self.dropout_param)\n",
    "            \n",
    "    ar_out, ar_cache[self.num_layers] = affine_forward(layer_input, self.params['W%d'%(self.num_layers)], self.params['b%d'%(self.num_layers)])\n",
    "    scores = ar_out\n",
    "    \n",
    "\n",
    "    # If test mode return early\n",
    "    if mode == 'test':\n",
    "      return scores\n",
    "\n",
    "    loss, dscores = softmax_loss(scores, y)\n",
    "    dhout = dscores\n",
    "    loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(self.num_layers)] * self.params['W%d'%(self.num_layers)])\n",
    "    \n",
    "    dx , dw , db = affine_backward(dhout , ar_cache[self.num_layers])\n",
    "    grads = {}\n",
    "    grads['W%d'%(self.num_layers)] = dw + self.reg * self.params['W%d'%(self.num_layers)]\n",
    "    grads['b%d'%(self.num_layers)] = db\n",
    "    \n",
    "    dhout = dx\n",
    "    for idx in xrange(self.num_layers-1):\n",
    "        lay = self.num_layers - 1 - idx - 1\n",
    "        loss = loss + 0.5 * self.reg * np.sum(self.params['W%d'%(lay+1)] * self.params['W%d'%(lay+1)])\n",
    "        if self.use_dropout:\n",
    "            dhout = dropout_backward(dhout ,dp_cache[lay])\n",
    "        if self.use_batchnorm:\n",
    "            dx, dw, db, dgamma, dbeta = affine_bn_relu_backward(dhout, ar_cache[lay])\n",
    "        else:\n",
    "            dx, dw, db = affine_relu_backward(dhout, ar_cache[lay])\n",
    "        grads['W%d'%(lay+1)] = dw + self.reg * self.params['W%d'%(lay+1)]\n",
    "        grads['b%d'%(lay+1)] = db\n",
    "        if self.use_batchnorm:\n",
    "            grads['gamma%d'%(lay+1)] = dgamma\n",
    "            grads['beta%d'%(lay+1)] = dbeta\n",
    "        dhout = dx\n",
    "\n",
    "    return loss, grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
