{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_forward(x, W):\n",
    "    \"\"\"\n",
    "    Forward pass for word embeddings. We operate on minibatches of size N where\n",
    "    each sequence has length T. We assume a vocabulary of V words, assigning each\n",
    "    to a vector of dimension D.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Integer array of shape (N, T) giving indices of words. Each element idx\n",
    "      of x muxt be in the range 0 <= idx < V.\n",
    "    - W: Weight matrix of shape (V, D) giving word vectors for all words.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Array of shape (N, T, D) giving word vectors for all input words.\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    " \n",
    "    # For each element in x we get its corresponding word embedding vector from W.\n",
    "    out = W[x, :]\n",
    "    cache = x, W\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def word_embedding_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for word embeddings. We cannot back-propagate into the words\n",
    "    since they are integers, so we only return gradient for the word embedding\n",
    "    matrix.\n",
    "\n",
    "    HINT: Look up the function np.add.at\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream gradients of shape (N, T, D)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    - dW: Gradient of word embedding matrix, of shape (V, D).\n",
    "    \"\"\"\n",
    "    x, W = cache\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    # Adds the upcoming gradients into the corresponding index from W.\n",
    "    np.add.at(dW, x, dout)\n",
    "    return dW\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    A numerically stable version of the logistic sigmoid function.\n",
    "    \"\"\"\n",
    "    pos_mask = (x >= 0)\n",
    "    neg_mask = (x < 0)\n",
    "    z = np.zeros_like(x)\n",
    "    z[pos_mask] = np.exp(-x[pos_mask])\n",
    "    z[neg_mask] = np.exp(x[neg_mask])\n",
    "    top = np.ones_like(x)\n",
    "    top[neg_mask] = z[neg_mask]\n",
    "    return top / (1 + z)\n",
    "\n",
    "def temporal_affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)\n",
    "    - w: Weights of shape (D, M)\n",
    "    - b: Biases of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data of shape (N, T, M)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    N, T, D = x.shape\n",
    "    M = b.shape[0]\n",
    "    out = x.reshape(N * T, D).dot(w).reshape(N, T, M) + b\n",
    "    cache = x, w, b, out\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def temporal_affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - dout: Upstream gradients of shape (N, T, M)\n",
    "    - cache: Values from forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input, of shape (N, T, D)\n",
    "    - dw: Gradient of weights, of shape (D, M)\n",
    "    - db: Gradient of biases, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b, out = cache\n",
    "    N, T, D = x.shape\n",
    "    M = b.shape[0]\n",
    "\n",
    "    dx = dout.reshape(N * T, M).dot(w.T).reshape(N, T, D)\n",
    "    dw = dout.reshape(N * T, M).T.dot(x.reshape(N * T, D)).T\n",
    "    db = dout.sum(axis=(0, 1))\n",
    "\n",
    "    return dx, dw, db\n",
    "\n",
    "def temporal_softmax_loss(x, y, mask, verbose=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: Input scores, of shape (N, T, V)\n",
    "    - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "         0 <= y[i, t] < V\n",
    "    - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "      the scores at x[i, t] should contribute to the loss.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving loss\n",
    "    - dx: Gradient of loss with respect to scores x.\n",
    "    \"\"\"\n",
    "\n",
    "    N, T, V = x.shape\n",
    "\n",
    "    x_flat = x.reshape(N * T, V)\n",
    "    y_flat = y.reshape(N * T)\n",
    "    mask_flat = mask.reshape(N * T)\n",
    "\n",
    "    probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "    dx_flat = probs.copy()\n",
    "    dx_flat[np.arange(N * T), y_flat] -= 1\n",
    "    dx_flat /= N\n",
    "    dx_flat *= mask_flat[:, None]\n",
    "\n",
    "    if verbose: print('dx_flat: ', dx_flat.shape)\n",
    "\n",
    "    dx = dx_flat.reshape(N, T, V)\n",
    "\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla_rnn_structure\n",
    "![](img/vanilla.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    x_Wx = np.dot(x, Wx)\n",
    "    prev_h_Wh = np.dot(prev_h, Wh)\n",
    "    # Calculate next hidden state = tanh(x*Wx + prev_H*Wh + b)\n",
    "    next_h = np.tanh(x_Wx + prev_h_Wh + b)\n",
    "    cache = (Wx, Wh, b, x, prev_h, next_h)\n",
    "    return next_h, cache\n",
    "\n",
    "\n",
    "def rnn_step_backward(dnext_h, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradient of loss with respect to next hidden state\n",
    "    - cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradients of bias vector, of shape (H,)\n",
    "    \"\"\"\n",
    "    \n",
    "    Wx, Wh, b, x, prev_h, next_h = cache\n",
    "\n",
    "    # Backprop dnext_h through the tanh function first, derivative of tanh is 1-tanh^2.\n",
    "    dtanh = (1 - np.square(next_h)) * dnext_h\n",
    "\n",
    "    # Backprop dtanh to calculate the other derivates (no complicated derivatives here).\n",
    "    db = np.sum(dtanh, axis=0)\n",
    "    dWh = np.dot(prev_h.T, dtanh)\n",
    "    dprev_h = np.dot(dtanh, Wh.T)\n",
    "    dWx = np.dot(x.T, dtanh)\n",
    "    dx = np.dot(dtanh, Wx.T)\n",
    "\n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla_rnn_forward_structure\n",
    "\n",
    "![](img/vanilla_forward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    N, T, D = x.shape\n",
    "    N, H = h0.shape\n",
    "\n",
    "    # Initialise h for holding our calculated hidden states.\n",
    "    h = np.zeros([N, T, H])\n",
    "    cache = []\n",
    "\n",
    "    # Our initial hidden state\n",
    "    prev_h = h0\n",
    "\n",
    "    # RNN forward for T time steps.\n",
    "    for t_step in range(T):\n",
    "        cur_x = x[:, t_step, :]\n",
    "        prev_h, cache_temp = rnn_step_forward(cur_x, prev_h, Wx, Wh, b)\n",
    "        h[:, t_step, :] = prev_h\n",
    "        cache.append(cache_temp)\n",
    "\n",
    "    return h, cache\n",
    "\n",
    "\n",
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for a vanilla RNN over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "\n",
    "    Wx, Wh, b, x, prev_h, next_h = cache[0]\n",
    "    N, T, H = dh.shape\n",
    "    D, H = Wx.shape\n",
    "\n",
    "    # Initialise gradients.\n",
    "    dx = np.zeros([N, T, D])\n",
    "    dWx = np.zeros_like(Wx)\n",
    "    dWh = np.zeros_like(Wh)\n",
    "    db = np.zeros_like(b)\n",
    "    dprev_h = np.zeros_like(prev_h)\n",
    "\n",
    "    # Backprop in time - start at last calculated time step and work back.\n",
    "    for t_step in reversed(range(T)):\n",
    "\n",
    "        # Add the current timestep upstream gradient to previous calculated dh\n",
    "        cur_dh = dprev_h + dh[:,t_step,:]\n",
    "\n",
    "        # Calculate gradients at this time step.\n",
    "        dx[:, t_step, :], dprev_h, dWx_temp, dWh_temp, db_temp = rnn_step_backward(cur_dh, cache[t_step])\n",
    "\n",
    "        # Add gradient contributions from each time step.\n",
    "        dWx += dWx_temp\n",
    "        dWh += dWh_temp\n",
    "        db += db_temp\n",
    "\n",
    "    # dh0 is the last hidden state gradient calculated.\n",
    "    dh0 = dprev_h\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This file defines layer types that are commonly used for recurrent neural\n",
    "networks.\n",
    "\"\"\"\n",
    "\n",
    "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Forward pass for a single timestep of an LSTM.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, D)\n",
    "    - prev_h: Previous hidden state, of shape (N, H)\n",
    "    - prev_c: previous cell state, of shape (N, H)\n",
    "    - Wx: Input-to-hidden weights, of shape (D, 4H)\n",
    "    - Wh: Hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - b: Biases, of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - next_c: Next cell state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for backward pass.\n",
    "    \"\"\"\n",
    "    next_h, next_c, cache = None, None, None\n",
    "\n",
    "    # Get H for slicing up activation vector A.\n",
    "    H = np.shape(prev_h)[1]\n",
    "\n",
    "    # Calculate activation vector A=x*Wx + prev_h*Wh + b.\n",
    "    a_vector = np.dot(x, Wx) + np.dot(prev_h, Wh) + b\n",
    "\n",
    "    # Slice activation vector to get the 4 parts of it: input/forget/output/block.\n",
    "    a_i = a_vector[:, 0:H]\n",
    "    a_f = a_vector[:, H:2*H]\n",
    "    a_o = a_vector[:, 2*H:3*H]\n",
    "    a_g = a_vector[:, 3*H:]\n",
    "\n",
    "    # Activation functions applied to our 4 gates.\n",
    "    input_gate = sigmoid(a_i)\n",
    "    forget_gate = sigmoid(a_f)\n",
    "    output_gate = sigmoid(a_o)\n",
    "    block_input = np.tanh(a_g)\n",
    "\n",
    "    # Calculate next cell state.\n",
    "    next_c = (forget_gate * prev_c) + (input_gate * block_input)\n",
    "\n",
    "    # Calculate next hidden state.\n",
    "    next_h = output_gate * np.tanh(next_c)\n",
    "\n",
    "    # Cache variables needed for backprop.\n",
    "    cache = (x, Wx, Wh, b, prev_h, prev_c, input_gate, forget_gate, output_gate, block_input, next_c, next_h)\n",
    "\n",
    "    return next_h, next_c, cache\n",
    "\n",
    "\n",
    "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of an LSTM.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradients of next hidden state, of shape (N, H)\n",
    "    - dnext_c: Gradients of next cell state, of shape (N, H)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input data, of shape (N, D)\n",
    "    - dprev_h: Gradient of previous hidden state, of shape (N, H)\n",
    "    - dprev_c: Gradient of previous cell state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "    dx, dh, dc, dWx, dWh, db = None, None, None, None, None, None\n",
    "\n",
    "    x, Wx, Wh, b, prev_h, prev_c, input_gate, forget_gate, output_gate, block_input, next_c, next_h = cache\n",
    "\n",
    "    # Backprop dnext_h through the multiply gate.\n",
    "    dh_tanh = dnext_h * output_gate\n",
    "    da_o_partial = dnext_h * np.tanh(next_c)\n",
    "\n",
    "    # Backprop dh_tanh through the tanh function.\n",
    "    dtanh = dh_tanh * (1 - np.square(np.tanh(next_c)))\n",
    "\n",
    "    # We add dnext_c and dtanh as during forward pass we split activation (gradients add up at forks).\n",
    "    dtanh_dc = (dnext_c + dtanh)\n",
    "\n",
    "    # Backprop dtanh_dc to calculate dprev_c.\n",
    "    dprev_c = dtanh_dc * forget_gate\n",
    "\n",
    "    # Backprop dtanh_dc towards each gate.\n",
    "    da_i_partial = dtanh_dc * block_input\n",
    "    da_g_partial = dtanh_dc * input_gate\n",
    "    da_f_partial = dtanh_dc * prev_c\n",
    "\n",
    "    # Backprop through gate activation functions to calculate gate derivatives.\n",
    "    da_i = input_gate*(1-input_gate) * da_i_partial\n",
    "    da_f = forget_gate*(1-forget_gate) * da_f_partial\n",
    "    da_o = output_gate*(1-output_gate) * da_o_partial\n",
    "    da_g = (1-np.square(block_input)) * da_g_partial\n",
    "\n",
    "    # Concatenate back up our 4 gate derivatives to get da_vector.\n",
    "    da_vector = np.concatenate((da_i, da_f, da_o, da_g), axis=1)\n",
    "\n",
    "    # Backprop da_vector to get remaining gradients.\n",
    "    db = np.sum(da_vector, axis=0)\n",
    "    dx = np.dot(da_vector, Wx.T)\n",
    "    dWx = np.dot(x.T, da_vector)\n",
    "    dprev_h = np.dot(da_vector, Wh.T)\n",
    "    dWh = np.dot(prev_h.T, da_vector)\n",
    "\n",
    "    return dx, dprev_h, dprev_c, dWx, dWh, db\n",
    "\n",
    "\n",
    "def lstm_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Forward pass for an LSTM over an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The LSTM uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the LSTM forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Note that the initial cell state is passed as input, but the initial cell\n",
    "    state is set to zero. Also note that the cell state is not returned; it is\n",
    "    an internal variable to the LSTM and is not accessed from outside.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, T, D)\n",
    "    - h0: Initial hidden state of shape (N, H)\n",
    "    - Wx: Weights for input-to-hidden connections, of shape (D, 4H)\n",
    "    - Wh: Weights for hidden-to-hidden connections, of shape (H, 4H)\n",
    "    - b: Biases of shape (4H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for all timesteps of all sequences, of shape (N, T, H)\n",
    "    - cache: Values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    h, cache = None, None\n",
    "\n",
    "    N, T, D = x.shape\n",
    "    N, H = h0.shape\n",
    "\n",
    "    cache = []\n",
    "    h = np.zeros([N, T, H])\n",
    "\n",
    "    # Set initial h and c states.\n",
    "    prev_h = h0\n",
    "    prev_c = np.zeros_like(h0)\n",
    "\n",
    "    for time_step in range(T):\n",
    "        prev_h, prev_c, cache_temp = lstm_step_forward(x[:,time_step,:], prev_h, prev_c, Wx, Wh, b)\n",
    "        h[:, time_step, :] = prev_h  # Store the hidden state for this time step.\n",
    "        cache.append(cache_temp)\n",
    "    \n",
    "    return h, cache\n",
    "\n",
    "\n",
    "def lstm_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for an LSTM over an entire sequence of data.]\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of hidden states, of shape (N, T, H)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of input data of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weight matrix of shape (D, 4H)\n",
    "    - dWh: Gradient of hidden-to-hidden weight matrix of shape (H, 4H)\n",
    "    - db: Gradient of biases, of shape (4H,)\n",
    "    \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "\n",
    "    x, Wx, Wh, b, prev_h, prev_c, input_gate, forget_gate, output_gate, block_input, next_c, next_h = cache[0]\n",
    "\n",
    "    N, T, H = dh.shape\n",
    "    D, _ = Wx.shape\n",
    "\n",
    "    dx = np.zeros([N, T, D])\n",
    "    dprev_h = np.zeros_like(prev_h)\n",
    "    dWx = np.zeros_like(Wx)\n",
    "    dWh = np.zeros_like(Wh)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    # Initial gradient for cell is all zero.\n",
    "    dprev_c = np.zeros_like(dprev_h)\n",
    "\n",
    "    for time_step in reversed(range(T)):\n",
    "\n",
    "        # Add the current timestep upstream gradient to previous calculated dh.\n",
    "        cur_dh = dprev_h + dh[:,time_step,:]\n",
    "\n",
    "        dx[:,time_step,:], dprev_h, dprev_c, dWx_temp, dWh_temp, db_temp = lstm_step_backward(cur_dh, dprev_c, cache[time_step])\n",
    "\n",
    "        # Add gradient contributions from each time step together.\n",
    "        db += db_temp\n",
    "        dWh += dWh_temp\n",
    "        dWx += dWx_temp\n",
    "\n",
    "    # dh0 is the last hidden state gradient calculated.\n",
    "    dh0 = dprev_h\n",
    "\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
