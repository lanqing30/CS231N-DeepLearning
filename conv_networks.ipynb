{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [1]:本文件仅供学习原理使用, 实际应用中由于速度太慢, 一般使用 Cpython 编写的 fast 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deep_networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width HH.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "    \n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    \n",
    "    h_prime = int(1 + (H + 2 * pad - HH) / stride)\n",
    "    w_prime = int(1 + (W + 2 * pad - WW) / stride)\n",
    "    result = np.random.rand(N, F, h_prime, w_prime)\n",
    "    \n",
    "    x_padding = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    \n",
    "    for w_index in range(w_prime):\n",
    "        for h in range(h_prime):\n",
    "            for f in range(F):\n",
    "                partial = x_padding[:,:, h * stride: h*stride+HH, w_index * stride: w_index * stride + WW]\n",
    "                curr_filter = w[f]\n",
    "                for n in range(N):\n",
    "                    result[n, f, h, w_index] = np.sum(partial[n] * curr_filter) + b[f]\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    out = result\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "\n",
    "    x, w, b, conv_param = cache\n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    N, F, h_prime, w_prime = dout.shape\n",
    "\n",
    "    stride = conv_param['stride']\n",
    "    pad = conv_param['pad']\n",
    "\n",
    "    db = np.zeros(b.shape)\n",
    "    dw = np.zeros(w.shape)\n",
    "    dx = np.zeros(x.shape)\n",
    "    \n",
    "    dx_pad = np.pad(dx, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "\n",
    "    # compute the gradient\n",
    "    for n in range(N):\n",
    "        for f in range(F):\n",
    "            for h_index in range(h_prime):\n",
    "                for w_index in range(w_prime):\n",
    "                    partial_dout = dout[n, f, h_index, w_index]\n",
    "                    partial_dx = dx_pad[n,:,h_index*stride: h_index* stride + HH, w_index*stride:w_index*stride+WW]\n",
    "                    partial_dx += w[f] * partial_dout\n",
    "                    dw[f] += x_pad[n,:,h_index*stride: h_index* stride + HH, w_index*stride:w_index*stride+WW] * partial_dout\n",
    "                    db[f] += partial_dout\n",
    "    dx = dx_pad[:, :, pad:-pad, pad:-pad]\n",
    "\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    N,C,H,W = x.shape\n",
    "    HH = pool_param['pool_height']\n",
    "    WW = pool_param['pool_width']\n",
    "    stride = pool_param['stride']\n",
    "    h_prime = int(1 + (H - HH) / stride)\n",
    "    w_prime = int(1 + (W - WW) / stride)\n",
    "\n",
    "    result = np.random.rand(N, C, h_prime, w_prime)\n",
    "\n",
    "    for h_index in range(h_prime):\n",
    "        for w_index in range(w_prime):\n",
    "            for c_index in range(C):\n",
    "                result[:,c_index,h_index, w_index] = np.max(x[:,c_index, h_index*stride:h_index*stride+HH, w_index*stride:w_index*stride+WW], axis=(1,2))\n",
    "    out = result\n",
    "\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    (x, pool_param) = cache\n",
    "\n",
    "    N,C,H,W = x.shape\n",
    "    HH = pool_param['pool_height']\n",
    "    WW = pool_param['pool_width']\n",
    "    stride = pool_param['stride']\n",
    "    h_prime = int(1 + (H - HH) / stride)\n",
    "    w_prime = int(1 + (W - WW) / stride)\n",
    "\n",
    "    result = np.random.rand(N, C, h_prime, w_prime)\n",
    "\n",
    "    dx = np.zeros(x.shape)\n",
    "    for n in range(N):\n",
    "        for h_index in range(h_prime):\n",
    "            for w_index in range(w_prime):\n",
    "                temp_x = x[n,:,h_index*stride:h_index*stride+HH, w_index*stride:w_index*stride+WW]\n",
    "                max_x = np.max(temp_x, axis=(1,2)).reshape(C, 1, 1)\n",
    "                mask = max_x == temp_x\n",
    "                temp_dout = dout[n, :, h_index, w_index].reshape(C,1,1) * mask\n",
    "                dx[n,:,h_index*stride:h_index*stride+HH, w_index*stride:w_index*stride+WW] = temp_dout\n",
    "\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "        old information is discarded completely at every time step, while\n",
    "        momentum=1 means that new information is never incorporated. The\n",
    "        default of momentum=0.9 should work well in most situations.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    N, C, H, W = x.shape\n",
    "    x_reshape = x.transpose(0,3,2,1).reshape(-1, C)\n",
    "    out_spacial, cache = batchnorm_forward(x_reshape, gamma, beta, bn_param)\n",
    "    out = out_spacial.reshape(N,W,H,C).transpose(0,3,2,1)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "\n",
    "    N, C, H, W = dout.shape\n",
    "    dx_spacial, dgamma, dbeta = batchnorm_backward(dout.transpose(0,3,2,1).reshape(-1, C), cache)\n",
    "    dx = dx_spacial.reshape(N, W, H, C).transpose(0,3,2,1)\n",
    "\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_forward(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A convenience layer that performs a convolution followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    a, conv_cache = conv_forward_naive(x, w, b, conv_param)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (conv_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def conv_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the conv-relu convenience layer.\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = conv_backward_naive(da, conv_cache)\n",
    "    return dx, dw, db\n",
    "\n",
    "####(2)####\n",
    "def conv_bn_relu_forward(x, w, b, gamma, beta, conv_param, bn_param):\n",
    "    a, conv_cache = conv_forward_naive(x, w, b, conv_param)\n",
    "    an, bn_cache = spatial_batchnorm_forward(a, gamma, beta, bn_param)\n",
    "    out, relu_cache = relu_forward(an)\n",
    "    cache = (conv_cache, bn_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def conv_bn_relu_backward(dout, cache):\n",
    "    conv_cache, bn_cache, relu_cache = cache\n",
    "    dan = relu_backward(dout, relu_cache)\n",
    "    da, dgamma, dbeta = spatial_batchnorm_backward(dan, bn_cache)\n",
    "    dx, dw, db = conv_backward_naive(da, conv_cache)\n",
    "    return dx, dw, db, dgamma, dbeta\n",
    "\n",
    "####(3)####\n",
    "def conv_relu_pool_forward(x, w, b, conv_param, pool_param):\n",
    "    \"\"\"\n",
    "    Convenience layer that performs a convolution, a ReLU, and a pool.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the convolutional layer\n",
    "    - w, b, conv_param: Weights and parameters for the convolutional layer\n",
    "    - pool_param: Parameters for the pooling layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the pooling layer\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, conv_cache = conv_forward_naive(x, w, b, conv_param)\n",
    "    s, relu_cache = relu_forward(a)\n",
    "    out, pool_cache = max_pool_forward_naive(s, pool_param)\n",
    "    cache = (conv_cache, relu_cache, pool_cache)\n",
    "    return out, cache\n",
    "\n",
    "def conv_relu_pool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the conv-relu-pool convenience layer\n",
    "    \"\"\"\n",
    "    conv_cache, relu_cache, pool_cache = cache\n",
    "    ds = max_pool_backward_naive(dout, pool_cache)\n",
    "    da = relu_backward(ds, relu_cache)\n",
    "    dx, dw, db = conv_backward_naive(da, conv_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the following architecture:\n",
    "\n",
    "    conv - relu - 2x2 max pool - affine - relu - affine - softmax\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n",
    "                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n",
    "                 dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer\n",
    "        - filter_size: Size of filters to use in the convolutional layer\n",
    "        - hidden_dim: Number of units to use in the fully-connected hidden layer\n",
    "        - num_classes: Number of scores to produce from the final affine layer.\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "        C, H, W = input_dim\n",
    "        self.params['W1'] = np.random.normal(0, weight_scale, (num_filters, C, filter_size, filter_size))\n",
    "        self.params['W2'] = np.random.normal(0, weight_scale, (int((H/2)*(W/2)*num_filters), hidden_dim))\n",
    "        self.params['W3'] = np.random.normal(0, weight_scale, (hidden_dim, num_classes))\n",
    "\n",
    "        self.params['b1'] = np.zeros(num_filters)\n",
    "        self.params['b2'] = np.zeros(hidden_dim)\n",
    "        self.params['b3'] = np.zeros(num_classes)\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "\n",
    "        # pass conv_param to the forward pass for the convolutional layer\n",
    "        filter_size = W1.shape[2]\n",
    "        conv_param = {'stride': 1, 'pad': (filter_size - 1) // 2}\n",
    "\n",
    "        # pass pool_param to the forward pass for the max-pooling layer\n",
    "        pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "        scores = None\n",
    "        \n",
    "        conv_out_1 , conv_cache = conv_relu_pool_forward(X, self.params['W1'], self.params['b1'], conv_param, pool_param)\n",
    "        hidden_out, hidden_cache = affine_relu_forward(conv_out_1, self.params['W2'], self.params['b2'])\n",
    "        scores, out_cache = affine_forward(hidden_out, self.params['W3'], self.params['b3'])\n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        \n",
    "        loss, dout = softmax_loss(scores, y)\n",
    "        dx_out, grads['W3'], grads['b3'] = affine_backward(dout, out_cache)\n",
    "        dx_hidden, grads['W2'], grads['b2'] = affine_relu_backward(dx_out, hidden_cache)\n",
    "        dx, grads['W1'], grads['b1'] = conv_relu_pool_backward(dx_hidden, conv_cache)\n",
    "\n",
    "        W1, W2, W3 = (self.params['W1'], self.params['W2'], self.params['W3'])\n",
    "\n",
    "        loss += self.reg * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\n",
    "        grads['W1'] += 2 * self.reg * W1\n",
    "        grads['W2'] += 2 * self.reg * W2\n",
    "        grads['W3'] += 2 * self.reg * W3\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
